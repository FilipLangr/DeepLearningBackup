{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    #print(batches2string([self._last_batch]))\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "['ate social ', 'ments faile', 'al park pho', 'ies index s', 'ess of cast', ' h provided', 'guage among', 'gers in dec', 'al media an', ' during the', 'known manuf', 'seven a wid', 's covering ', 'en one of t', 'ze single a', ' first card', ' in jersey ', 'he poverty ', 'gns of huma', ' cause so a', 'n denatural', 'ce formatio', 'the input u', 'ck to pull ', 'usion inabi', 'omplete an ', 't of the mi', ' it fort de', 'ttempts by ', 'ormats for ', 'soteric chr', 'growing pop', 'riginal doc', 'e nine eigh', 'rch eight l', 'haracter li', 'al mechanic', ' gm compari', 's fundament', 'lieve the c', 'ast not par', ' upon by hi', ' example rl', 'ed on the w', 'he official', 'on at this ', 'ne three tw', 'inux enterp', ' daily coll', 'ration camp', 'ehru wished', 'stiff from ', 'arman s syd', 'o to begin ', 'itiatives t', 'these autho', 'icky ricard', 'w of mathem', 'ent of arm ', 'credited pr', 'e external ', ' other stat', ' buddhism e', 'vices possi']\n",
      "[' a']\n",
      "['an']\n",
      "['na']\n",
      "['ar']\n"
     ]
    }
   ],
   "source": [
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-9cc35379a73c>:81: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        # i.shape == (batch_size, vocabulary_size)\n",
    "        # o.shape == (batch_size, num_nodes)\n",
    "        # PROBLEM 1 single matrix multiply.\n",
    "        problem1 = True\n",
    "        if problem1:\n",
    "            i_boss = tf.matmul(i, tf.concat([ix, fx, cx, ox], 1))\n",
    "            o_boss = tf.matmul(o, tf.concat([im, fm, cm, om], 1))\n",
    "            iix, ifx, icx, iox = tf.split(i_boss, 4, 1)\n",
    "            oim, ofm, ocm, oom = tf.split(o_boss, 4, 1)\n",
    "            \n",
    "            input_gate = tf.sigmoid(iix + oim + ib)\n",
    "            forget_gate = tf.sigmoid(ifx + ofm + fb)\n",
    "            update = icx + ocm + cb\n",
    "            state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "            output_gate = tf.sigmoid(iox + oom + ob)\n",
    "        else:\n",
    "            input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "            forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "            update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "            state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "            output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "          tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.292127 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.90\n",
      "================================================================================\n",
      "hgnh o okg iui thdoopegwmvpclsgr lw  kla tt nn fdrvn usif svvnnxeacal ie iczzcaj\n",
      "mto emiitserppalsi nqpbwos hlrif  ek eiwlao el re lhhmudtrelvrpaccvqtahnzzq aux \n",
      "ti suhrx jelcijksjtbherfk mrnn bcizfetty iqlh adgrltivn  bmwwegtyd  knftqxdzrnbp\n",
      "lisahsrcxujn roli yjwgfzjeie w pa l eurpcjaxieyti  au njjkr ji tzjqettneacfuc  r\n",
      "d ibfsocxt fg osfdwtp ukoio gugl iqwlpkvhmobsdidere p colitcq nnwieiang hzzwicj \n",
      "================================================================================\n",
      "Validation set perplexity: 19.88\n",
      "Average loss at step 100: 2.597252 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.18\n",
      "Validation set perplexity: 10.10\n",
      "Average loss at step 200: 2.243014 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.88\n",
      "Validation set perplexity: 8.48\n",
      "Average loss at step 300: 2.096780 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.68\n",
      "Validation set perplexity: 7.55\n",
      "Average loss at step 400: 1.993560 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.99\n",
      "Validation set perplexity: 7.55\n",
      "Average loss at step 500: 1.931642 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.06\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 600: 1.904793 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 700: 1.852249 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.64\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 800: 1.811758 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 900: 1.824404 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 6.18\n",
      "Average loss at step 1000: 1.825486 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.48\n",
      "================================================================================\n",
      " an the fleatla of keramanted afon the bemone neon term ucia the defeftres behen\n",
      "y lequzai mont deniting is other dy selistici to the bean sos is hiveo s moretua\n",
      "fo his fohraple the mana the sill nomenter sycous afonga the dung it from hud an\n",
      "ur se us a in the secon yamd tive of deecar in the ford for encount ederikute s \n",
      "boh neh ecip sequution withie main the dich around dear woll the lest becometer \n",
      "================================================================================\n",
      "Validation set perplexity: 6.10\n",
      "Average loss at step 1100: 1.774516 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 5.85\n",
      "Average loss at step 1200: 1.751265 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 5.67\n",
      "Average loss at step 1300: 1.728795 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 1400: 1.742739 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 1500: 1.737785 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 1600: 1.737925 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 1700: 1.707566 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 1800: 1.674513 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 1900: 1.646779 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 2000: 1.693312 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "================================================================================\n",
      " a positist betwupm of the instamine st take unst and one one four a viet he tha\n",
      "diced an actronering insticulic for below tod when mitterras of ha accertory nam\n",
      "y servestndring and nocner keving woddvall as lagy he bitsules of reamanthalge f\n",
      "fowere for imirsl to pierxgles inllican crearing bane and maken purcodes by teen\n",
      "re wate has wad in the no pood to mepaly amarizent six nine foun one sine five s\n",
      "================================================================================\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 2100: 1.686845 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 2200: 1.675041 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 2300: 1.641857 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 2400: 1.656613 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 2500: 1.678850 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 2600: 1.652703 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 2700: 1.661757 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 2800: 1.646194 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 2900: 1.647518 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3000: 1.647248 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "================================================================================\n",
      "ft thoorgers and theord hopers diston in the coned sparting to the present corte\n",
      "kbbinor kearce it elaver and dooly is one nine six nine one nine seven nine nine\n",
      "posing deopen theirshned blown ver of that papusins of my itrass one ficiner and\n",
      "mained arous to kint his must mottoon rancted staule unt also memed is and weet \n",
      "jos is like thoungerswhan sports ena somilizing adodian simaled reary the bordam\n",
      "================================================================================\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 3100: 1.625299 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 3200: 1.644288 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 3300: 1.634206 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3400: 1.667609 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 3500: 1.654660 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 3600: 1.669615 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 3700: 1.645631 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 3800: 1.637976 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 3900: 1.637180 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 4000: 1.650207 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "================================================================================\n",
      "n repalson grently mugrop games to or belied under one two two four or canol gom\n",
      "man classifs in the maints with in over and a pravain bull in ot an intermetiste\n",
      "grects edments by their refinitivida in ansocomed in three karrists of varional \n",
      "ftamine in angavated at theremes of peadonizes elgy sit or croawn one zero zero \n",
      "urce flembli leavbals of lation of study the were cerends theare elegred they wo\n",
      "================================================================================\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 4100: 1.631591 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 4200: 1.636957 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 4300: 1.614615 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 4400: 1.609120 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4500: 1.615023 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 4600: 1.617826 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 4700: 1.619597 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4800: 1.631305 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 4900: 1.630675 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 5000: 1.608547 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "================================================================================\n",
      "hy spensons reagtlia the that the was of accorting cassicely zero hes itiant re \n",
      "venyram lood from the canix being bexase and there selfy an epmanged heldy compl\n",
      "ver they the one nine one two zero zero zero nishng as the his americs one six r\n",
      "f based the novementilites states runi mooth two are the eachs open to boyture e\n",
      "its of state is back him godole mach woust some impired with as been tonather be\n",
      "================================================================================\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5100: 1.604307 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 5200: 1.590943 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 5300: 1.577066 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 5400: 1.573107 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 5500: 1.563196 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 5600: 1.580547 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 5700: 1.570222 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5800: 1.577847 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5900: 1.578676 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 6000: 1.547627 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.32\n",
      "================================================================================\n",
      "jounn a beat fo zero nine zero zero one of the golommy more armined judation a j\n",
      "s number the gount vapter england six anylomber delevse of reconding dettrion br\n",
      "noceling the call aopressadial mainst blua seased hed pablicm rebebser from pay \n",
      "use of also stingt feitner persons scondrom it critical seang while supposering \n",
      "m desnifian ennmease of the are indown into are hazne the yastoum nocides offeme\n",
      "================================================================================\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 6100: 1.568694 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6200: 1.536141 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.13\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 6300: 1.541206 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 6400: 1.540728 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 6500: 1.557058 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 6600: 1.593369 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6700: 1.578879 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 6800: 1.603155 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.10\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 6900: 1.581445 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 7000: 1.579089 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "================================================================================\n",
      "juriominally systemees seoms expurchuce one what list of whomsome to claysilian \n",
      "ety agai si c seven one three st back tribonuly chals not one rite res pokesh de\n",
      "z subersion was instanctural i ophe applicistativelances into collections one fo\n",
      "m guecaltonia are jespopal soger elever himserf beconsold written as between see\n",
      "finity or it gradumain the use the garulec gequal ayonics borma pligge aimed by \n",
      "================================================================================\n",
      "Validation set perplexity: 4.34\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See code above with problem1=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Embedding.\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        # i.shape == (batch_size, embedding_size)\n",
    "        # o.shape == (batch_size, num_nodes)\n",
    "        i_boss = tf.matmul(i, tf.concat([ix, fx, cx, ox], 1))\n",
    "        o_boss = tf.matmul(o, tf.concat([im, fm, cm, om], 1))\n",
    "        iix, ifx, icx, iox = tf.split(i_boss, 4, 1)\n",
    "        oim, ofm, ocm, oom = tf.split(o_boss, 4, 1)\n",
    "\n",
    "        input_gate = tf.sigmoid(iix + oim + ib)\n",
    "        forget_gate = tf.sigmoid(ifx + ofm + fb)\n",
    "        update = icx + ocm + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(iox + oom + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    def embed_me(inp):\n",
    "        # embeddings.shape: (vocabulary_size, embedding_size)\n",
    "        # inp.shape: (batch_size, vocabulary_size)\n",
    "        # inp contains 1-hot-encoding data.\n",
    "        # tf.argmax(inp, dimension=1) takes index of \"the 1\" in a row of 1 and many 0's.\n",
    "        # we obtain tensor with shape: (batch_size,) that looks up in embeddings\n",
    "        return tf.nn.embedding_lookup(embeddings, tf.argmax(inp, dimension=1))\n",
    "    \n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "          # TODO float32 ???\n",
    "          tf.placeholder(tf.int32, shape=[batch_size, vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        i = embed_me(i)\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf.concat(train_labels, 0)))\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(embed_me(sample_input), saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.301189 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.14\n",
      "================================================================================\n",
      "pjhh zliw wbwmnye wtia citls ispuxdce a aneoe  cqrkdt slpkmdtaaffe edh xm e  yte\n",
      "ogye u nonmqypnwa ytarphlba grsga p f dgy  cirabgsia piqho a ne csu  bio bmnjypu\n",
      "pld xwrrifte nl n wzkrbmaqolofiwe luieriqoiidoihiayy sajio hlgrcyfcs bl iu s git\n",
      "e duzonsv bnyiwgbdysdkvhibciyciyenceiigc  lliktienls  ug e m  m m vp im ilve gqj\n",
      "t an o bdxqev    odrkcmi e cshnqxont t se nlm  ckonjar  e pvonyi  e clqhxff ddcj\n",
      "================================================================================\n",
      "Validation set perplexity: 19.55\n",
      "Average loss at step 100: 2.279729 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.77\n",
      "Validation set perplexity: 8.59\n",
      "Average loss at step 200: 1.999902 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.73\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 300: 1.919552 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 6.25\n",
      "Average loss at step 400: 1.856156 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 6.27\n",
      "Average loss at step 500: 1.822309 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Validation set perplexity: 5.94\n",
      "Average loss at step 600: 1.810678 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 6.02\n",
      "Average loss at step 700: 1.771579 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 5.66\n",
      "Average loss at step 800: 1.734150 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 5.60\n",
      "Average loss at step 900: 1.752361 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 1000: 1.760944 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.03\n",
      "================================================================================\n",
      "ge leatlear reganed a rus can zero by system infurtwork whl as adound tware of a\n",
      "atcil awad ecrace actent pars internoty victor west the sit the hetwead to the s\n",
      "ariah duscrips speel its use usea and her his has tiam histens have provader an \n",
      "s againd humaguan bastle sequric the simit beat writienmus but two seur on taeki\n",
      "er a yupputing sekes som the gart moused the as commoning sold anood one three a\n",
      "================================================================================\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 1100: 1.718720 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 1200: 1.702272 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 1300: 1.681469 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 1400: 1.698646 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 1500: 1.695030 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 1600: 1.708666 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 1700: 1.679248 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 1800: 1.642805 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 1900: 1.617875 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 2000: 1.663501 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "================================================================================\n",
      "er was boot yeragent laker the stater pidopog glaced courch the diddines to moil\n",
      "ying languan bottwo and mtucka im as good manstonic of miding which canthern for\n",
      "ble tabrett somimeth fourft duristan day for longeentherbua but incomeded space \n",
      "y bon in nine whore refive not or in and all and was howlities universt of is hi\n",
      "juctih sitely s noth an jon in the cractures shone jowrough formable in year a w\n",
      "================================================================================\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 2100: 1.658055 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 2200: 1.651640 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 2300: 1.621820 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 2400: 1.640166 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 2500: 1.669365 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 2600: 1.640689 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 2700: 1.651175 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 2800: 1.635982 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 2900: 1.641181 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 3000: 1.642147 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "================================================================================\n",
      "ms distiction often and and histon posula ruly chastronsly murath such large lan\n",
      "t obsced ninking fourw to s malegpage that hold to pops fooned ably be one he on\n",
      "ke termpur flank formally for the comply with have legind his amerying and cy th\n",
      "wars one nine severe persified pssan codeake as election for hand desuch a s par\n",
      "quel of zero was severy but nine ned pearced through condiddity lands one seven \n",
      "================================================================================\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3100: 1.621076 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 3200: 1.641572 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 3300: 1.630426 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 3400: 1.663929 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 3500: 1.654310 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 3600: 1.679826 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3700: 1.645191 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 3800: 1.645681 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 3900: 1.640464 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 4000: 1.657922 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "================================================================================\n",
      "jer in the undi dun peups book energy of although me gold otforbal is united sif\n",
      "ganogondier altuchs hug for the furns and popeadom the finited of enstance be vo\n",
      "fer cong amfedo erimection juing idle president his at croced of one nine three \n",
      " pricting the remains s been acomed houp three zero zero mnatic fel way fave aim\n",
      "x and zero two six the sonow war aleding there dooth mor the and and scalaum  bl\n",
      "================================================================================\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4100: 1.636811 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 4200: 1.642129 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 4300: 1.621850 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 4400: 1.616842 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 4500: 1.623592 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 4600: 1.625765 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4700: 1.636823 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4800: 1.645004 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 4900: 1.641997 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 5000: 1.619373 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "================================================================================\n",
      "litary one one seven nine three as they  esplemepy bordentana state interedaloni\n",
      "ypted there autotesta music as kodnicates ncomed house that fiowable there s two\n",
      "ving and ofkan could buttly they ving the stitution these by three three zero fo\n",
      "hic to consool s the mostler bought muselay decomed these once it very by the po\n",
      "viage due wassemot dick history the have both is berhen to biltwhattre the geash\n",
      "================================================================================\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5100: 1.593675 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 5200: 1.567818 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 5300: 1.552862 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.30\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 5400: 1.555189 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 5500: 1.543702 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.22\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 5600: 1.556131 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 5700: 1.550978 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.26\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 5800: 1.550378 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.30\n",
      "Validation set perplexity: 4.13\n",
      "Average loss at step 5900: 1.551362 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.11\n",
      "Average loss at step 6000: 1.522381 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.25\n",
      "================================================================================\n",
      "phes are has deas acture macrove botten in latrs lets over his membruties usebo \n",
      "b of could tclenbounts a one seven to pluding suntay also one nine soicial unknu\n",
      "onent leed turby two zero one zero gr univer general dibit of dation amal holden\n",
      "s footwor unitral regreadifians and physicial indiard be artiliked used are thos\n",
      "gita insturary durs neasals often one nine eight one cunies all on morred in by \n",
      "================================================================================\n",
      "Validation set perplexity: 4.12\n",
      "Average loss at step 6100: 1.542475 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.12\n",
      "Average loss at step 6200: 1.513783 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.06\n",
      "Validation set perplexity: 4.11\n",
      "Average loss at step 6300: 1.526343 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.06\n",
      "Average loss at step 6400: 1.520680 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.03\n",
      "Average loss at step 6500: 1.536004 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.05\n",
      "Average loss at step 6600: 1.576572 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.05\n",
      "Average loss at step 6700: 1.558791 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.08\n",
      "Average loss at step 6800: 1.581657 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.09\n",
      "Validation set perplexity: 4.10\n",
      "Average loss at step 6900: 1.558880 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 7000: 1.551607 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "================================================================================\n",
      "s the have watter intoles in as the far faily bost behonices by while as iif of \n",
      "rednu in uttox a centled syntic had miscurey mastelied transsingly for kinvan in\n",
      "x meassey soundarcumed thro see gen numerature in d hav b jemor their meas seard\n",
      "re are machies on latterlen three a gau milismissio food and haffaspress follow \n",
      "p or to datter hands jame names consider of through gulanth of etcase and unlish\n",
      "================================================================================\n",
      "Validation set perplexity: 4.09\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) + b) +  optionaly c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "\n",
    "wanna_dropout=True\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Embedding.\n",
    "    # PROBLEM 2 b).\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size**2, embedding_size], -1.0, 1.0))\n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    # Problem 2 c) dropout.\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        # i.shape == (batch_size, embedding_size)\n",
    "        # o.shape == (batch_size, num_nodes)\n",
    "        \n",
    "        # Problem 2 c) dropout.\n",
    "        if wanna_dropout:\n",
    "            i = tf.nn.dropout(i, keep_prob)\n",
    "        i_boss = tf.matmul(i, tf.concat([ix, fx, cx, ox], 1))\n",
    "        o_boss = tf.matmul(o, tf.concat([im, fm, cm, om], 1))\n",
    "        iix, ifx, icx, iox = tf.split(i_boss, 4, 1)\n",
    "        oim, ofm, ocm, oom = tf.split(o_boss, 4, 1)\n",
    "\n",
    "        input_gate = tf.sigmoid(iix + oim + ib)\n",
    "        forget_gate = tf.sigmoid(ifx + ofm + fb)\n",
    "        update = icx + ocm + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(iox + oom + ob)\n",
    "        # Problem 2 c) dropout.\n",
    "        if wanna_dropout:\n",
    "            output_dropped = tf.nn.dropout(output_gate * tf.tanh(state), keep_prob)\n",
    "        else:\n",
    "            output_dropped = output_gate * tf.tanh(state)\n",
    "        return output_dropped, state\n",
    "    \n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "          tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "    # PROBLEM 2 b).\n",
    "    # Make pairs of characters.\n",
    "    train_inputs = zip(train_data[:num_unrollings-1], train_data[1:num_unrollings])\n",
    "    train_labels = train_data[2:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        # PROBLEM 2 b).\n",
    "        # embeddings.shape: (vocabulary_size**2, embedding_size)\n",
    "        # i looks up in embeddings. We have 2 chars, all the combinations give vocabulary_size**2 dim space.\n",
    "        i = tf.argmax(i[0], dimension=1) + vocabulary_size * tf.argmax(i[1], dimension=1)\n",
    "        i = tf.nn.embedding_lookup(embeddings, i)\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf.concat(train_labels, 0)))\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    # PROBLEM 2 b).\n",
    "    input01 = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    input02 = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    bigram_input_predictions = tf.argmax(input01, dimension=1) + vocabulary_size * tf.argmax(input02, dimension=1)\n",
    "    lstm_input = tf.nn.embedding_lookup(embeddings, bigram_input_predictions)\n",
    "    \n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    # PROBLEM 2 b).\n",
    "    sample_output, sample_state = lstm_cell(lstm_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.305392 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.26\n",
      "================================================================================\n",
      "veepivycyqanayaiafenytzisowzktbgt urmnntzo mviqibtkatg jotnqfnn ipi cpnlsk fcz an\n",
      "qgfdaodsvl orukpdilbdniettsg dnem ut  tspetphtnuliihlm hgaaqnosrpmfmv il b reann \n",
      "ujbhriheh ygy s tvt njdeoeouef q fsmcnot rmwmjdyc eita hhea rtatlssv n zhwtikifn \n",
      "kl wcfmvbcetaimgr s tbyro s ai nnea xextdmkhf z ai zvrsg sacoyh qastrats ng s fgi\n",
      "bb zahsbflbvnj qtitloqu d  z etng kt gelyaht iabaaz ocwydsrfselio r  o e rfuerys \n",
      "================================================================================\n",
      "Validation set perplexity: 20.11\n",
      "Average loss at step 100: 2.488104 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.87\n",
      "Validation set perplexity: 9.44\n",
      "Average loss at step 200: 2.199077 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.36\n",
      "Validation set perplexity: 8.43\n",
      "Average loss at step 300: 2.131307 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.64\n",
      "Validation set perplexity: 7.96\n",
      "Average loss at step 400: 2.077568 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.81\n",
      "Validation set perplexity: 7.84\n",
      "Average loss at step 500: 2.051139 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.19\n",
      "Validation set perplexity: 7.49\n",
      "Average loss at step 600: 2.049160 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.84\n",
      "Validation set perplexity: 7.75\n",
      "Average loss at step 700: 2.013539 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.73\n",
      "Validation set perplexity: 7.56\n",
      "Average loss at step 800: 1.986037 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.10\n",
      "Validation set perplexity: 7.60\n",
      "Average loss at step 900: 2.012578 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.31\n",
      "Validation set perplexity: 7.59\n",
      "Average loss at step 1000: 2.020413 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.73\n",
      "================================================================================\n",
      "mims dever sing distile with poslink expenews depisces palrigintere etworka seven\n",
      "lmte dosr a the it cal ruseriguoseven of the resodnne ext eiliseterstrial by bely\n",
      "bside mary of viar mugular feveme of have mys the be a the exn liborol and covers\n",
      "czing first adhi and he lugtions lanis three pection liva comple process s sucati\n",
      "qque sdet age kno thew be three riciffachipprepeathe treed of bee of mased nehmes\n",
      "================================================================================\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 1100: 1.984378 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.30\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 1200: 1.967977 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.09\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 1300: 1.948699 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.51\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 1400: 1.971717 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 1500: 1.980723 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.66\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 1600: 1.981902 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.92\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 1700: 1.956041 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.74\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 1800: 1.924659 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 1900: 1.897438 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.80\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 2000: 1.949808 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.10\n",
      "================================================================================\n",
      "jhrebrosit was ust nanlely whrocel of formadeustroces psary fal intem worke aortu\n",
      "specially in armuary on namerid dizero neven the gliwtch moremcs courstolon stati\n",
      "ycpvcial hine wordbsicentor film bope which dtyles ladegonveir wide dized aroum t\n",
      " op don boin of whosistent mhadeade pearken for foles deare new one six c molckle\n",
      "vt see exterbinary of ferns as x ageath and were one to nor all chentorn morads w\n",
      "================================================================================\n",
      "Validation set perplexity: 7.44\n",
      "Average loss at step 2100: 1.961515 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.77\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 2200: 1.942844 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 2300: 1.910380 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.59\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 2400: 1.931151 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.77\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 2500: 1.943907 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 2600: 1.927245 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.62\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 2700: 1.935896 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 2800: 1.929469 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.36\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 2900: 1.927090 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 3000: 1.926543 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.26\n",
      "================================================================================\n",
      "one ruuages a gord the mlasourgunce from the unciprited ective at medoags belijec\n",
      "p to snoften or deniceed mith the saia g knowyr severs as assilc which at with ev\n",
      "cally that omr s one nine nine five i most shilinks at of beluccem not estationat\n",
      "tkeformate proncome it of elrtly haveive the bove unia intes pychancass and the a\n",
      "nver whit standpuptionos bealled to for of and fakrnificra seven meanly and one n\n",
      "================================================================================\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 3100: 1.902404 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.12\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 3200: 1.925224 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.83\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 3300: 1.918821 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.66\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 3400: 1.954238 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.46\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 3500: 1.941793 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.58\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 3600: 1.949299 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.03\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 3700: 1.932570 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.70\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 3800: 1.918676 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 3900: 1.922292 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.80\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 4000: 1.929986 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.17\n",
      "================================================================================\n",
      "mpel datly key and canotly majlvite stentovelfe year mansition cated dayr ho and \n",
      "nbes the wimple and mijrreral inshafter lez has of as sub kalkox yopize and in af\n",
      "qkrs slual onzent in in regeda dispece eithely with jated centurartary and with i\n",
      "ahimanays secomefficial the tba one zero seven and one nine manlinter disctop act\n",
      "ykl in mart of gelop p words reopospecuth with its the be of had volj acm were in\n",
      "================================================================================\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 4100: 1.913846 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.36\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 4200: 1.919213 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.07\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 4300: 1.907871 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.93\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 4400: 1.899449 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.51\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 4500: 1.900447 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 4600: 1.903662 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.53\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 4700: 1.921080 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 4800: 1.921287 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.86\n",
      "Validation set perplexity: 7.24\n",
      "Average loss at step 4900: 1.920277 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 5000: 1.881291 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.69\n",
      "================================================================================\n",
      "qeath irt treek dout intakeir the dor they largotemather conteen wyce ahis bresca\n",
      "hde or seave the and stositone line six artnity is nee coutp muneyeight whecholes\n",
      "czations and jith s fariates a cervaeuyed laregince the notent to his comd brober\n",
      "ized bized reminic one thrigine nine six six peathors from the event one seven ei\n",
      "fwcwosed as ofsais into one zero zero it five zero in the lite pdital heass disso\n",
      "================================================================================\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 5100: 1.900028 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.82\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 5200: 1.897719 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.98\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 5300: 1.876618 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 5400: 1.866945 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.81\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 5500: 1.872478 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 5600: 1.873441 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.42\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 5700: 1.872471 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 5800: 1.869443 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 5900: 1.876330 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.55\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 6000: 1.843679 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.78\n",
      "================================================================================\n",
      "scrang bh in of dayern of comportent s reasity elot plays a cupictory they diest \n",
      " from ranceen jo batten to his teasuguary expebseconner give two zero zero one s \n",
      "hysation the the nudies the laters by aind one eight michad has four stado idosap\n",
      "kroudqtter lonamil supportels that mainly in the were the partrack deesel dantall\n",
      "sjeworld of conf one nine urd tranzed to abome emirations to one worids for times\n",
      "================================================================================\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 6100: 1.858938 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.94\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 6200: 1.835160 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 6300: 1.840300 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.51\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 6400: 1.830419 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.18\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 6500: 1.851420 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 6600: 1.888017 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.28\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 6700: 1.868155 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.38\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 6800: 1.895922 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 6900: 1.866009 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 7000: 1.863722 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.76\n",
      "================================================================================\n",
      "tjecty lina exxother eticul of schykims in the unly vide c one seven terweption q\n",
      "jderns jasians is while were and viliozanb fews ats wat dah for hicups bcde and t\n",
      "ave as in e regions ages it or coses besel the mod altre four to raelohens usetic\n",
      "rres vurpiet objectiii saocals becausing the unfeaker two five nine binan much it\n",
      "yhd ware the sing filbeth post one nine five three eight six four group late boel\n",
      "================================================================================\n",
      "Validation set perplexity: 6.83\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "# PROBLEM 2 b).\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    # Problem 2 c) dropout.\n",
    "    feed_dict = {keep_prob: 0.7}\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      # PROBLEM 2 b).\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          # PROBLEM 2 b).\n",
    "          preds1 = random_distribution()\n",
    "          preds2 = random_distribution()\n",
    "          sentence = characters(preds1)[0] + characters(preds2)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            # Predictions on train set.\n",
    "            # Problem 2 c) dropout.\n",
    "            prediction = sample_prediction.eval({input01: preds1, input02: preds2, keep_prob: 1.0})\n",
    "            preds1 = preds2\n",
    "            preds2 = sample(prediction)\n",
    "            sentence += characters(preds2)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        # Predictions on valid set.\n",
    "        # PROBLEM 2 b), c) dropout.\n",
    "        predictions = sample_prediction.eval({input01: b[0], input02: b[1], keep_prob: 1.0})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dropout False: Validation set perplexity: 6.49\n",
    "\n",
    "dropout True, keep_prob=0.5: Validation set perplexity: 7.38\n",
    "\n",
    "dropout True, keep_prob=0.6: Validation set perplexity: 7.02\n",
    "\n",
    "dropout True, keep_prob=0.7: Validation set perplexity: 6.83"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
